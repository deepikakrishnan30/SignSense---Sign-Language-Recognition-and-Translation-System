Sign language recognition and translation systems play a crucial role in facilitating communication between the deaf or hard of hearing community and the hearing population. This paper proposes a novel approach for sign language recognition and translation to text, coupled with voice synthesis, using Keras for model development and Pyttsx3 for voice synthesis. The system comprises two main components: sign language recognition and translation to text, followed by text-to-speech synthesis. For sign language recognition, a convolutional neural network (CNN) model is trained on a dataset of sign language gestures to accurately classify and interpret hand gestures. Keras, a high-level neural networks API, is utilized for efficient model development and training. Subsequently, the recognized gestures are translated into text using sequence-to-sequence models or similar techniques. Finally, the translated text is converted into spoken language using Pyttsx3, a Python library for text-to-speech conversion, enabling seamless communication between sign language users and non-signers. Experimental results demonstrate the effectiveness and accuracy of the proposed system in real-time sign language recognition, translation to text, and synthesized voice output. This integrated system holds promise for enhancing accessibility and inclusivity for the deaf and hard of hearing community in various communication contexts.

![image](https://github.com/user-attachments/assets/6e80bab3-e7bf-4e00-8a0f-4cd764d38734)
